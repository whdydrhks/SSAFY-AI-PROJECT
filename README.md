# ğŸ—¿ AI ğŸ©º ì‚¬ìš© ëª¨ë¸ : KoBERT / KoGPT2
---

## ğŸ›¹ KoBERT ?

- Bert ëª¨ë¸ì˜ í•œêµ­ì–´ ë²„ì „ìœ¼ë¡œ, SKTì—ì„œ  êµ¬ê¸€ì˜ BERT ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤. 

- ìì—°ì–´ ì²˜ë¦¬(NLP)ê°€ ê°€ëŠ¥í•œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.

- í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹, ë¬¸ì¥ ìœ ì‚¬ë„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ë„¤ì´ë²„ì—ì„œ ê³µê°œí•œ í•œê¸€ ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„°ë¥¼ ì‚¬ì „ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.

- [KoBERT ê¹ƒí—ˆë¸Œ](https://github.com/SKTBrain/KoBERT)

--- 
## ğŸ›¶ BERT?
- BERTëŠ” "Bidirectional Encoder Representations from Transformers"ì˜ ì•½ì–´ë¡œ, êµ¬ê¸€ì—ì„œ ê°œë°œí•œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. 
- BERTì˜ ì¥ì ì¸ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë¬¸ì¥ ë¶„ë¥˜, ë¬¸ì¥ ìœ ì‚¬ë„, ì§ˆë¬¸ ì‘ë‹µ ë“±ì˜ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

- Transformerë¼ëŠ” ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 
    * TransformerëŠ” Attention mechanismì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ëœ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì˜ì¡´ ê´€ê³„ë¥¼ ì°¾ì•„ë‚´ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.
- [BERT ê¹ƒí—ˆë¸Œ](https://github.com/google-research/bert)

---

## ğŸ›« KoGPT2 ?

- KoGPT2ëŠ” SKTì—ì„œ ê°œë°œí•œ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ GPT ëª¨ë¸ì…ë‹ˆë‹¤. GPTëŠ” Generative Pre-trained Transformerì˜ ì•½ìë¡œ, íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±ì— ëŒ€í•œ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

- KoGPT2ëŠ” í•œêµ­ì–´ ë¬¸ì¥ì— ëŒ€í•œ í† í°í™”(tokenization), ì„ë² ë”©(embedding), GPT ëª¨ë¸ í•™ìŠµ ë° í…ìŠ¤íŠ¸ ìƒì„±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ëª¨ë“ˆë“¤ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ì–‘í•œ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- KoGPT2ë¥¼ ì´ìš©í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´, ë¨¼ì € ë¬¸ì¥ì„ í† í°í™”í•˜ê³  KoGPT ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ì˜ˆì¸¡ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´í›„, ì–»ì€ ì˜ˆì¸¡ê°’ì„ ì´ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- [KoGPT2 ê¹ƒí—ˆë¸Œ](https://github.com/SKT-AI/KoGPT2)
---

## â›µ GPT2
- GPT-2 (Generative Pre-trained Transformer 2)ëŠ” OpenAIì—ì„œ ê°œë°œí•œ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì…ë‹ˆë‹¤. 
- GPT-2ëŠ” Transformer ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ë©°, í…ìŠ¤íŠ¸ ìƒì„±, ê¸°ê³„ ë²ˆì—­, ì§ˆì˜ ì‘ë‹µ, ìš”ì•½, ê°ì„± ë¶„ì„ ë“± ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ëŒ€í•´ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

- [GPT-2 ê¹ƒí—ˆë¸Œ](https://github.com/openai/gpt-2)

---

### âš“ **ì„¤ì¹˜íŒŒì¼** ###
```text
pip install -r requirements.txt
```
- tokenizerì˜ ê²½ìš°,
```text
!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf
```

## ğŸ’¡ **ê°ì • ë¶„ì„ ğŸ‘‰ KoBERT**

### âœï¸ **Process**

1. **ì…ë ¥ ë°ì´í„° ì „ ì²˜ë¦¬**
    - ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ í† í° IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë¬¸ì¥ì˜ ì‹¤ì œ ê¸¸ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ë¬¸ì¥ ìŒì˜ ê²½ìš°, ë¬¸ì¥ ê°„ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” ì„¸ê·¸ë¨¼íŠ¸ IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. **BERT ëª¨ë¸ ì ìš©**
    - ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°ì´í„°ë¥¼ BERT ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì¶œë ¥ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
    - BERT ëª¨ë¸ì€ ì…ë ¥ í† í°ì„ ì„ë² ë”©í•˜ê³ , ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê° í† í°ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    - ì¶œë ¥ê°’ì€ BERTì˜ ì¶œë ¥ ì°¨ì› í¬ê¸°ì¸ 768ì°¨ì›ìœ¼ë¡œ ë‚˜ì˜µë‹ˆë‹¤.
3. **ì¶œë ¥ê°’ ë³€í™˜**
    - BERT ëª¨ë¸ì˜ ì¶œë ¥ê°’ ì¤‘ ë¬¸ì¥ì„ ëŒ€í‘œí•˜ëŠ” poolerë¥¼ ì‚¬ìš©í•˜ì—¬ 768ì°¨ì› ë²¡í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    - ë²¡í„°ë¥¼ ì„ í˜• ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ë ˆì´ë¸”ì— ëŒ€í•œ ë¡œì§“ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ë¡œì§“ì€ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
4. **ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™”**
    - logitê³¼ ì‹¤ì œ í´ë˜ìŠ¤ ë ˆì´ë¸” ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
    - í•™ìŠµì—ëŠ” í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent)ì˜ ë³€í˜• ì¤‘ í•˜ë‚˜ì¸ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - í•™ìŠµë¥ ê³¼ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ë“±ì„ ì¡°ì •í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.

### ğŸ§¾ Dataset
![ê°ì •ë¶„ì„](https://user-images.githubusercontent.com/109534450/229683701-ceb7d12e-81bc-478a-8efb-519c14c7ad9c.PNG)
- [AI Hub ê°ì„± ëŒ€í™” ë§ë­‰ì¹˜ ë°ì´í„° ì…‹](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=86)
- ì¶”ê°€ ë°ì´í„° ìƒì„± ë° í•™ìŠµ

### ğŸ›  Requirements

```
!pip install gluonnlp pandas tqdm
!pip install mxnet
!pip install sentencepiece
!pip install transformers
!pip install torch
!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'

```

### âš¾ï¸ Import

```
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook
import pandas as pd
from sklearn.model_selection import train_test_split
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

from google.colab import drive
drive.mount('/content/drive')

```

- **torch**
- **nn, optim, F, Dataset, DataLoader :** íŒŒì´í† ì¹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í•˜ìœ„ ëª¨ë“ˆ, ì‹ ê²½ë§ ëª¨ë¸, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í•¨ìˆ˜, ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë”
- **gluonnlp :** MXNet í”„ë ˆì„ ì›Œí¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **numpy :** ë‹¤ì°¨ì› í–‰ë ¬ ê´€ë ¨
- **pandas :** ë°ì´í„° ì²˜ë¦¬
- **KoBERTTokenizer :** KoBERTì˜ í† í¬ë‚˜ì´ì €
- **BertModel**
- **AdamW :** AdamW ìµœì í™”
- **get_cosine_schedule_with_warmup :** learning rate ìŠ¤ì¼€ì¤„ë§ ì ìš© í•¨ìˆ˜
- **drive.mount('/content/drive/') :** êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ Colabì— ë™ê¸°í™”
- **device = torch.device("cuda:0") :** GPU ì‚¬ìš©

### ğŸ¾ Hyper Parameter

```
# Setting parameters
max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 50
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5
```

***í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì…‹íŒ…***

- **max_len** ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´
- **batch_size** í•œ ë²ˆì˜ batchë§ˆë‹¤ ì£¼ëŠ” ë°ì´í„° ìƒ˜í”Œì˜ size. 
- **num_epochs :** ì „ì²´ í•™ìŠµ ë°ì´í„° í•™ìŠµ íšŸìˆ˜
- **learning_rate :** í•™ìŠµë¥ 

### ğŸ§šâ€â™€ï¸ BERT Dataset

```
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,
                 pad, pair):

        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))
```

- gluonnlpì˜ BERTSentenceTransformì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë¬¸ì¥ì„ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
-transform ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì¥ì„ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜

### ğŸ§šâ€â™‚ï¸ BERT Classifier

```
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=6,
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate

        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)

    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)

        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)

```

- ì…ë ¥ ë°ì´í„°ì˜ íŒ¨ë”© ë¶€ë¶„ì„ ì œì™¸í•˜ê³ , ì‹¤ì œ ì…ë ¥ì— ëŒ€í•œ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
- token_idsëŠ” ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•œ ê²°ê³¼
- gen_attention_mask ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±
- BERT ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ì—¬ ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

### ğŸ”« í† í¬ ë‚˜ì´ì € / ëª¨ë¸ ì •ì˜

```
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')
tok = tokenizer.tokenize
```

- skt/kobert-base-v1 ëª¨ë¸ì˜ **ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜** ë¡œë“œ
- BERTVocab ê°ì²´ì— tokenizer.vocab_file ì‚¬ì „(vocab) ë¡œë“œ

### ğŸ’¿ ë°ì´í„° ë¡œë“œ/ ì „ì²˜ë¦¬

```
train_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…¡á†·á„Œá…¥á†¼á„‡á…®á†«á„‰á…¥á†¨dataset.csv', encoding='cp949')
validation_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Validation.csv',encoding='cp949')
train_set = train_set.loc[:, ['sentiment', 'user']]
validation_set = validation_set.loc[:, ['ê°ì •_ëŒ€ë¶„ë¥˜', 'ì‚¬ëŒë¬¸ì¥1']]

train_set.dropna(inplace=True)
validation_set.dropna(inplace=True)
train_set.columns = ['label', 'data']
validation_set.columns = ['label', 'data']

train_set.loc[(train_set['label'] == 'ì¼ìƒ'), 'label'] = 0
train_set.loc[(train_set['label'] == 'ë¶„ë…¸'), 'label'] = 1
train_set.loc[(train_set['label'] == 'ë¶ˆì•ˆ'), 'label'] = 2
train_set.loc[(train_set['label'] == 'ìŠ¬í””'), 'label'] = 3
train_set.loc[(train_set['label'] == 'ê¸°ì¨'), 'label'] = 4
train_set.loc[(train_set['label'] == 'ìš°ìš¸'), 'label'] = 5

validation_set.loc[(validation_set['label'] == 'ì¼ìƒ'), 'label'] = 0
validation_set.loc[(validation_set['label'] == 'ë¶„ë…¸'), 'label'] = 1
validation_set.loc[(validation_set['label'] == 'ë¶ˆì•ˆ'), 'label'] = 2
validation_set.loc[(validation_set['label'] == 'ìŠ¬í””'), 'label'] = 3
validation_set.loc[(validation_set['label'] == 'ê¸°ì¨'), 'label'] = 4
validation_set.loc[(validation_set['label'] == 'ìš°ìš¸'), 'label'] = 5

train_set_data = [[i, str(j)] for i, j in zip(train_set['data'], train_set['label'])]

# validation_set_data = [[i, str(j)] for i, j in zip(validation_set['data'], validation_set['label'])]

train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=4)
train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)
test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)
train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=2)
test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=2)

```

### ğŸ§¸ ëª¨ë¸ í•™ìŠµ

```
model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)
# Prepare optimizer and schedule (linear warmup and decay)
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()
t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)

for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()  # Update learning rate schedule
        train_acc += calc_accuracy(out, label)
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
    print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
    model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        test_acc += calc_accuracy(out, label)
    print("epoch {} test acc {}".format(e+1, test_acc / (batch_id+1)))

```

**train ê³¼ validation ì§„í–‰**

### **í•™ìŠµ ì„¤ì •ê°’**

- BERTClassifier í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ëª¨ë¸ì€ BERT ëª¨ë¸ê³¼ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ë¡œ êµ¬ì„±
- AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
- ê°€ì¤‘ì¹˜ ê°ì‡ (weight decay)ë¥¼ ì ìš©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
- ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì ìš©í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ì™€ ì ìš©í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ë¶„í•˜ì—¬ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •
- CrossEntropyLoss í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

### **í•™ìŠµ ë™ì‘ê³¼ì • (for ~)**

- ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
- token_ids, segment_ids, valid_length, label ê°’ì„ GPU ì˜¬ë¦¬ê³ , modelì— input -> out
- out ê°’ê³¼ label ê°’ì„ ì‚¬ìš©í•˜ì—¬ Cross Entropy Lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
- loss.backward() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ëª¨ë¸ì˜ ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°
- optimizer.step() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ ê°’ì„ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
- CosineAnnealingWarmRestarts ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©, í•™ìŠµë¥ ì„ ì¡°ì •
- í˜„ì¬ ë°°ì¹˜ì˜ í•™ìŠµ ì •í™•ë„ train_accì— ëˆ„ì 
- í˜„ì¬ epochì˜ í•™ìŠµ ì •í™•ë„ë¥¼ ì¶œë ¥

### **ğŸ§¬ ê°ì •**
**ğŸ˜€ì¼ìƒ - 0 ğŸ˜Šê¸°ì¨ -1 ğŸ˜§ë¶ˆì•ˆ - 2 ğŸ˜­ìŠ¬í”” - 3 ğŸ˜¡ë¶„ë…¸ - 4 ğŸ˜¥ìš°ìš¸ - 5**

<img src = "https://user-images.githubusercontent.com/109534450/229018209-f9fb7af0-0800-47f8-981b-3ea3e13d2e9d.png" width="55%" height="55%">


### ğŸ“¤ Predict

```
def calc_accuracy(X,Y):
    max_vals, max_indices = torch.max(X, 1)
    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
    return train_acc
def predict(sentence):
    dataset = [[sentence, '0']]
    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)
    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=2)
    model.eval()
    answer = 0
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        for logits in out:
            logits = logits.detach().cpu().numpy()
            answer = np.argmax(logits)
    return answer
```

- predict í•¨ìˆ˜ëŠ” ì…ë ¥ ë¬¸ì¥ì„ ì…ë ¥ ë°›ì•„ í•´ë‹¹ ë¬¸ì¥ ê°ì • ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
- ì…ë ¥ ë¬¸ì¥ì„ datasetì— ì¶”ê°€í•˜ì—¬ BERTDataset ê°ì²´ë¥¼ ìƒì„±
- ëª¨ë¸ í‰ê°€ ëª¨ë“œ(model.eval())
- DataLoaderì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
- ëª¨ë¸ ì¶œë ¥ ê°’ì—ì„œ ê°€ì¥ í° ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ ê°’ìœ¼ë¡œ ë°˜í™˜

### ğŸ“© **OUT**
<img src = "https://user-images.githubusercontent.com/109534450/229065068-57d1e4f1-b9f1-461c-a0d2-d8a1d966d20f.png" width="55%" height="55%">

### âœ¨ ê²°ê³¼ë¬¼
**Loss**
- CrossEntropyLoss() - êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
- <img src = "https://user-images.githubusercontent.com/109534450/229016538-98df19eb-1bd3-4f78-8aff-abf498ca8759.png" width="35%" height="35%">
- ë‘Â í™•ë¥ Â ë¶„í¬ì˜Â ì°¨ì´ë¥¼Â êµ¬í•˜ê¸°Â ìœ„í•´ì„œÂ ì‚¬ìš©ë©ë‹ˆë‹¤.
- ì‹¤ì œÂ ë°ì´í„°ì˜Â í™•ë¥ Â ë¶„í¬ì™€, í•™ìŠµëœ ëª¨ë¸ì´Â ê³„ì‚°í•œ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.

<img src = "https://user-images.githubusercontent.com/109534450/229390889-8b9a2114-675f-4529-aacf-29a2fbd19055.png" width="55%" height="55%">



**Accuracy**


<img src = "https://user-images.githubusercontent.com/109534450/229020020-d61be612-8721-4348-9961-390a18955766.png" width="55%" height="55%">


**Confusion Matrix**


<img src = "https://user-images.githubusercontent.com/109534450/229709741-e4cbb9bd-9d79-4d3c-8180-ae4e9e5bed35.png" width="55%" height="55%">


---

---
## ğŸ’¡ **ê°ì„± ì±—ë´‡ ğŸ‘‰ KoGPT2**
### âœï¸ **Process**

1. ë°ì´í„° ì „ì²˜ë¦¬

- ëŒ€í™” ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ í† í° IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
ì§ˆë¬¸ê³¼ ëŒ€ë‹µ ê°„ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
ë¬¸ì¥ ìŒì˜ ê²½ìš°, ë¬¸ì¥ ê°„ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°, ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ ì˜ë¼ëƒ…ë‹ˆë‹¤.
2. koGPT ëª¨ë¸ ì ìš©

- ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°ì´í„°ë¥¼ KoGPT2 ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì¶œë ¥ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
KoGPT2 ëª¨ë¸ì€ ì…ë ¥ í† í°ì„ ì„ë² ë”©í•˜ê³ , ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê° í† í°ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
ì¶œë ¥ê°’ì€ KoGPT2 ëª¨ë¸ì˜ ì¶œë ¥ ì°¨ì› í¬ê¸°ì™€ ë™ì¼í•œ í¬ê¸°ì˜ ë¡œì§“ìœ¼ë¡œ ë‚˜ì˜µë‹ˆë‹¤.
3. ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™”

- logitê³¼ ì‹¤ì œ ë ˆì´ë¸” ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
í•™ìŠµì—ëŠ” Adam ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
í•™ìŠµë¥ ê³¼ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ë“±ì„ ì¡°ì •í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
4. Predict

- KoGPT2 ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì…ë ¥ëœ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
ì¸ì½”ë”©ëœ input_idsë¥¼ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í°ì„ ì„ íƒí•˜ì—¬ ëŒ€ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

### ğŸ§¾ **Dataset**
![bert_ì±—ë´‡](https://user-images.githubusercontent.com/109534450/229683525-159091de-309a-417f-8749-5754cffc37a1.PNG)
- [ê°ì„± ëŒ€í™” ë§ë­‰ì¹˜ ë°ì´í„° ì…‹ (AI Hub)](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=86)
- ì¶”ê°€ ë°ì´í„° ìƒì„± ë° í•™ìŠµ

### ğŸ›  **Requirements**
```
! pip install transformers
! pip install pytorch-lightning
! pip install torch
```

### âš¾ï¸ **Import**

```
import numpy as np
import pandas as pd
import torch
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning import LightningModule
from torch.utils.data import DataLoader, Dataset
from transformers.optimization import AdamW, get_cosine_schedule_with_warmup
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
import re

```
### **ğŸ¥Œ í† í°**
```python
Q_TKN = "<usr>"
A_TKN = "<sys>"
BOS = '</s>'
EOS = '</s>'
MASK = '<unused0>'
SENT = '<unused1>'
PAD = '<pad>
```
- Q_TKN = "<usr>" : ì§ˆë¬¸ ìœ ì € í† í°
- A_TKN = "<sys>" : ëŒ€ë‹µ ì‹œìŠ¤í…œ í† í°
- BOS = '</s>' : ë¬¸ì¥ì˜ ì‹œì‘
- EOS = '</s>' : ë¬¸ì¥ì˜ ëì„
- MASK = '<unused0>' : ë§ˆìŠ¤í¬ ì²˜ë¦¬
- SENT = '<unused1>' : ë¬¸ì¥ ì²˜ë¦¬
- PAD = '<pad>' : íŒ¨ë”© 

### ğŸ¾ **Hyper Parameter**

```
# Setting parameters
learning_rate = 3e-5
criterion = torch.nn.CrossEntropyLoss(reduction="mean")
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
epoch = 70
Sneg = -1e18
```

***í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì…‹íŒ…***
- criterion :  CrossEntropyLoss 
- optimizer : Adam 

### ğŸ§šâ€â™€ï¸ **KoGPT2 Chatbot Dataset**

```python 
class ChatbotDataset(Dataset):
    def __init__(self, chats, max_len=100):  # ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ë¶€ë¶„
        self._data = chats
        self.max_len = max_len
        self.q_token = Q_TKN
        self.a_token = A_TKN
        self.sent_token = SENT
        self.eos = EOS
        self.mask = MASK
        self.tokenizer = koGPT2_TOKENIZER

    def __len__(self):  # chatbotdata ì˜ ê¸¸ì´ë¥¼ ë¦¬í„´í•œë‹¤.
        return len(self._data)

    def __getitem__(self, idx):  # ë¡œë“œí•œ ì±—ë´‡ ë°ì´í„°ë¥¼ ì°¨ë¡€ì°¨ë¡€ DataLoaderë¡œ ë„˜ê²¨ì£¼ëŠ” ë©”ì„œë“œ
        turn = self._data.iloc[idx]
        q = turn["ì‚¬ëŒë¬¸ì¥1"]  # ì§ˆë¬¸ì„ ê°€ì ¸ì˜¨ë‹¤.
        q = re.sub(r"([?.!,])", r" ", q)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.

        a = turn["ì‹œìŠ¤í…œë¬¸ì¥1"]  # ë‹µë³€ì„ ê°€ì ¸ì˜¨ë‹¤.
        a = re.sub(r"([?.!,])", r" ", a)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.

        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)
        q_len = len(q_toked)

        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)
        a_len = len(a_toked)

        #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´
        if q_len > self.max_len:
            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´
                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ 
                q_len = len(q_toked)
                a_len = self.max_len - q_len              #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            a_toked = a_toked[:a_len]
            a_len = len(a_toked)

        #ì§ˆë¬¸ì˜ ê¸¸ì´ + ë‹µë³€ì˜ ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´
        if q_len + a_len > self.max_len:
            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´
                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ 
                q_len = len(q_toked)
                a_len = self.max_len - q_len              #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            a_toked = a_toked[:a_len]
            a_len = len(a_toked)

        # ë‹µë³€ labels = [mask, mask, ...., mask, ..., <bos>,..ë‹µë³€.. <eos>, <pad>....]
        labels = [self.mask,] * q_len + a_toked[1:]

        # mask = ì§ˆë¬¸ê¸¸ì´ 0 + ë‹µë³€ê¸¸ì´ 1 + ë‚˜ë¨¸ì§€ 0
        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)
        # ë‹µë³€ labelsì„ index ë¡œ ë§Œë“ ë‹¤.
        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)
        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING
        while len(labels_ids) < self.max_len:
            labels_ids += [self.tokenizer.pad_token_id]

        # ì§ˆë¬¸ + ë‹µë³€ì„ index ë¡œ ë§Œë“ ë‹¤.    
        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)
        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING
        while len(token_ids) < self.max_len:
            token_ids += [self.tokenizer.pad_token_id]

        #ì§ˆë¬¸+ë‹µë³€, ë§ˆìŠ¤í¬, ë‹µë³€
        return (token_ids, np.array(mask), labels_ids)

def collate_batch(batch):
    data = [item[0] for item in batch]
    mask = [item[1] for item in batch]
    label = [item[2] for item in batch]
    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)
```


### ğŸ”« **í† í¬ ë‚˜ì´ì € / ëª¨ë¸ ì •ì˜**

```python
koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
            bos_token=BOS, eos_token=EOS, unk_token='<unk>',
            pad_token=PAD, mask_token=MASK) 
model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
```

- skt/kogpt2-base-v2 ëª¨ë¸ì— ëŒ€í•œ í† í¬ë‚˜ì´ì € ë¡œë“œ
- GPT2LMHeadModelì€ GPT-2 ëª¨ë¸ ë¡œë“œ, 'skt/kogpt2-base-v2' ëª¨ë¸

### ğŸ’¿ **ë°ì´í„° ë¡œë“œ/ ì „ì²˜ë¦¬**

```python
import urllib.request

Chatbot_Data = pd.read_csv("./data/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.csv", encoding="cp949")
Chatbot_Data = Chatbot_Data[:51629]
Chatbot_Data.head()
train_set = ChatbotDataset(Chatbot_Data, max_len=100)
train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)
```

### ğŸ§¸ **ëª¨ë¸ í•™ìŠµ**

```python
import matplotlib.pyplot as plt

losses = []
X = []
Y = []
print("start")
for epoch in range(epoch):
    print(epoch)
    X.append(epoch)
    for batch_idx, samples in enumerate(train_dataloader):
        optimizer.zero_grad()
        token_ids, mask, label = samples
        token_ids = token_ids.to(device)
        mask = mask.to(device)
        label = label.to(device)
        out = model(token_ids)        
        out = out.logits.to(device)
        
        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2).to(device)
        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out)).to(device)
        loss = criterion(mask_out.transpose(2, 1), label).to(device)
        
        avg_loss = loss.sum() / mask.sum()
        avg_loss.backward()
        optimizer.step()
    Y.append(loss.data.cpu().numpy())
print("end")

```
- train_dataloaderì—ì„œ token_ids, mask, label ê°’ì„ í• ë‹¹

- optimizerë¥¼ ì´ˆê¸°í™”í•˜ê³ , ëª¨ë¸ì˜ ì¶œë ¥ ê°’ outì„ ê³„ì‚°, logits ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ ë¡œì§“ ê°’ì„ ê³„ì‚°

- mask ê°’ì„ 3ì°¨ì›ìœ¼ë¡œ í™•ì¥, outê³¼ ë™ì¼í•œ í¬ê¸°ë¡œ ë°˜ë³µí•˜ì—¬ mask_3dë¥¼ ìƒì„±.
- torch.where ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ maskê°€ 1ì¸ ìœ„ì¹˜ëŠ” out ê°’ì„, 0ì¸ ìœ„ì¹˜ëŠ” Negative infinity ê°’ì„ ê°€ì§€ëŠ” í…ì„œë¥¼ ìƒì„±í•˜ì—¬ mask_out ë³€ìˆ˜ì— í• ë‹¹

- ì†ì‹¤ í•¨ìˆ˜(criterion)ë¥¼ ê³„ì‚°í•˜ê³ , loss ê°’ì„ ì´ìš©í•˜ì—¬ í˜„ì¬ ì†ì‹¤ ê°’ì„ êµ¬í•©ë‹ˆë‹¤. ì´ë•Œ, ì†ì‹¤ ê°’ì˜ í‰ê· ì„ êµ¬í•˜ê¸° ìœ„í•´ avg_loss ë³€ìˆ˜ë¥¼ ê³„ì‚°

- backward ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê³ , step ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸

### ğŸ“¤ **Predict**

```python
def kogpt(input_text):
    q = input_text
    a = ""
    sent = ""
    while True:
        input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + sent + A_TKN + a)).unsqueeze(dim=0)
        pred = model(input_ids)
        pred = pred.logits
        gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().tolist())[-1]
        if gen == EOS:
            break
        a += gen.replace("â–", " ")
    return a
```

-  koGPT2 ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì…ë ¥ëœ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
- ì¸ì½”ë”©ëœ input_idsë¥¼ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ pred ë³€ìˆ˜ì— ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥
- torch.argmax í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í°ì„ ì„ íƒí•˜ê³ , ì´ë¥¼ gen ë³€ìˆ˜ì— ì €ì¥, ì´ë•Œ convert_ids_to_tokens í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì„ íƒëœ í† í°ì„ ë¬¸ìì—´ë¡œ ë³€í™˜

- gen ë³€ìˆ˜ê°€ EOSì¸ ê²½ìš°, ìƒì„± ê³¼ì •ì„ ë§ˆì¹¨. ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° genì— ëŒ€í•œ ê°’ì„ aì— ì¶”ê°€

### ğŸ“© **OUT**

```
input_text = "ì˜¤ëŠ˜ ê°‘ìê¸° ë‚ ì”¨ê°€ ì¶”ì›Œì ¸ì„œ ë†€ëì–´"
kogpt(sentence) # "ë‚ ì”¨ê°€ ë§ì´ ì¶”ì›Œì¡Œêµ°ìš”"
```



## ğŸ’¡ **ê°ì„± ì±—ë´‡ ğŸ‘‰ KoBERT**
### âœï¸ **Process**

- **ë°ì´í„° ë¡œë“œ** : 
ì›°ë‹ˆìŠ¤ ìƒë‹´ ì¹´í…Œê³ ë¦¬ì™€ ëŒ€ë‹µ ë°ì´í„°ë¥¼ íŒŒì¼ì—ì„œ ì½ê³ , ì¹´í…Œê³ ë¦¬ì™€ ëŒ€ë‹µì— ëŒ€í•œ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” load_wellness_answer() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤.

- **ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ìƒì„±** : WellnessTextClassificationDataset í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  í•™ìŠµ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°ì´í„° ë¡œë”ë¥¼ ë§Œë“¤ì–´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”**: KoBERTforSequenceClassification ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³ , ì„ í˜• ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ê¸° ìœ„í•´ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.

- **í•™ìŠµ í•¨ìˆ˜ êµ¬í˜„** : train() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì—í­ë§ˆë‹¤ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©° ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ì™€ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

- **í•™ìŠµ ì‹¤í–‰**: ì§€ì •ëœ ì—í­ ìˆ˜ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ì •ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— í•™ìŠµì„ ì´ì–´ì„œ í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

- **ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ**: ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµëœ KoBERT ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê³ , í† í¬ë‚˜ì´ì €ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

- **ì‚¬ìš©ì** : ì‚¬ìš©ìë¡œë¶€í„° ì§ˆë¬¸ì„ ì…ë ¥ë°›ê³ , ì…ë ¥ëœ ì§ˆë¬¸ì„ ì „ì²˜ë¦¬í•œ í›„ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
ì˜ˆì¸¡ëœ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ëŒ€ë‹µ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤.


### ğŸ§¾ **ë°ì´í„° ì…‹**
![bert_ì±—ë´‡ëŒ€í™”](https://user-images.githubusercontent.com/109534450/229684303-4779a711-26b8-41ac-bcb2-cc60e2470555.PNG)
- AIÂ HubÂ ì œê³µ,Â ì›°ë‹ˆìŠ¤Â ëŒ€í™”Â ìŠ¤í¬ë¦½íŠ¸Â ë°ì´í„°ì…‹ (_í˜„ì¬ í˜ì´ì§€ ì—†ìŒ_)

### âš¾ï¸ **classifier**

```python 
import torch
import torch.nn as nn
from kobert_transformers import get_kobert_model
from torch.nn import CrossEntropyLoss, MSELoss
from transformers import BertPreTrainedModel

import logging

from transformers import BertConfig

logger = logging.getLogger(__name__)

#KoBERT
kobert_config = {
    'attention_probs_dropout_prob': 0.1,
    'hidden_act': 'gelu',
    'hidden_dropout_prob': 0.1,
    'hidden_size': 768,
    'initializer_range': 0.02,
    'intermediate_size': 3072,
    'max_position_embeddings': 512,
    'num_attention_heads': 12,
    'num_hidden_layers': 12,
    'type_vocab_size': 2,
    'vocab_size': 8002
}

def get_kobert_config():
    return BertConfig.from_dict(kobert_config)


class KoBERTforSequenceClassfication(BertPreTrainedModel):
    def __init__(self,
                 num_labels=359,
                 hidden_size=768,
                 hidden_dropout_prob=0.1,
                 ):
        super().__init__(get_kobert_config())

        self.num_labels = num_labels
        self.kobert = get_kobert_model()
        self.dropout = nn.Dropout(hidden_dropout_prob)
        self.classifier = nn.Linear(hidden_size, num_labels)
        self.init_weights()
    def forward(
            self,
            input_ids=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            labels=None,
    ):
        outputs = self.kobert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
        )
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        outputs = (logits,) + outputs[2:] 

        if labels is not None:
            if self.num_labels == 1:
                #  We are doing regression
                loss_fct = MSELoss()
                loss = loss_fct(logits.view(-1), labels.view(-1))
            else:
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            outputs = (loss,) + outputs
        return outputs  # (loss), logits, (hidden_states), (attentions)


def kobert_input(tokenizer, str, device=None, max_seq_len=512):
    index_of_words = tokenizer.encode(str)
    token_type_ids = [0] * len(index_of_words)
    attention_mask = [1] * len(index_of_words)

    # Padding Length
    padding_length = max_seq_len - len(index_of_words)

    # Zero Padding
    index_of_words += [0] * padding_length
    token_type_ids += [0] * padding_length
    attention_mask += [0] * padding_length

    data = {
        'input_ids': torch.tensor([index_of_words]).to(device),
        'token_type_ids': torch.tensor([token_type_ids]).to(device),
        'attention_mask': torch.tensor([attention_mask]).to(device),
    }
    return data
```
- kobert_transformersë¥¼ ì‚¬ìš©í•˜ì—¬ **KoBERT ëª¨ë¸** ê°€ì ¸ì˜¤ê¸°
- ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” **KoBERTforSequenceClassfication** í´ë˜ìŠ¤ë¥¼ ì •ì˜
#### **kobert_input** : 
- ë¬¸ì¥ì„ í† í°í™”í•˜ê³ , íŒ¨ë”©ì„ ì ìš©í•œ í›„, í† í° ìœ í˜• IDì™€ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì„¤ì •
- ëª¨ë¸ì— ì „ë‹¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜

#### **KoBERTforSequenceClassfication** :
- __init__ (self, num_labels, hidden_size, hidden_dropout_prob): í´ë˜ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì„¤ì •
- forward : ì…ë ¥ í…ì„œë¥¼ ë°›ì•„ ëª¨ë¸ì„ í†µí•´ ì „ë‹¬, ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë°˜í™˜. ì†ì‹¤ ë°˜í™˜


### ğŸ§šâ€â™€ï¸ **dataloder**
```python
import torch
from kobert_transformers import get_tokenizer
from torch.utils.data import Dataset


class WellnessTextClassificationDataset(Dataset):
    def __init__(self,
                 file_path="/home/jupyter-j8b101/kobert-wellness-chatbot/data/wellness_dialog_for_text_classification_all.txt",
                 num_label=359,
                 device='cuda:8',
                 max_seq_len=512,  # KoBERT max_length
                 tokenizer=None
                 ):
        self.file_path = file_path
        self.device = device
        self.data = []
        self.tokenizer = tokenizer if tokenizer is not None else get_tokenizer()

        file = open(self.file_path, 'r', encoding='utf-8')

        while True:
            line = file.readline()
            if not line:
                break
            datas = line.split("    ")
            index_of_words = self.tokenizer.encode(datas[0])
            token_type_ids = [0] * len(index_of_words)
            attention_mask = [1] * len(index_of_words)

            # Padding Length
            padding_length = max_seq_len - len(index_of_words)

            # Zero Padding
            index_of_words += [0] * padding_length
            token_type_ids += [0] * padding_length
            attention_mask += [0] * padding_length

            # Label
            label = int(datas[1][:-1])

            data = {
                'input_ids': torch.tensor(index_of_words).to(self.device),
                'token_type_ids': torch.tensor(token_type_ids).to(self.device),
                'attention_mask': torch.tensor(attention_mask).to(self.device),
                'labels': torch.tensor(label).to(self.device)
            }
            self.data.append(data)
        file.close()

    def __len__(self):
        return len(self.data)
    def __getitem__(self, index):
        item = self.data[index]
        return item

if __name__ == "__main__":
    dataset = WellnessTextClassificationDataset()
    print(dataset)
```
- **WellnessTextClassificationDataset**
-  ì „ì²˜ë¦¬ 


## **Train**

### :rabbit: **import**
```python
import gc
import os

import numpy as np
import torch
from torch.utils.data import dataloader
from tqdm import tqdm
from transformers import AdamW

from model.classifier import KoBERTforSequenceClassfication
from model.dataloader import WellnessTextClassificationDataset

import matplotlib.pyplot as plt
```

### :four_leaf_clover: **train** 

```python
def train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step=0):
    losses = []
    train_start_index = train_step + 1 if train_step != 0 else 0
    total_train_step = len(train_loader)
    model.train()
    with tqdm(total=total_train_step, desc=f"Train({epoch})") as pbar:
        pbar.update(train_step)
        for i, data in enumerate(train_loader, train_start_index):
            optimizer.zero_grad()
            outputs = model(**data)
            loss = outputs[0]
            losses.append(loss.item())
            loss.backward()
            optimizer.step()
            pbar.update(1)
            pbar.set_postfix_str(f"Loss: {loss.item():.3f} ({np.mean(losses):.3f})")
            if i >= total_train_step or i % save_step == 0:
                torch.save({
                    'epoch': epoch,  # í˜„ì¬ í•™ìŠµ epoch
                    'model_state_dict': model.state_dict(),  # ëª¨ë¸ ì €ì¥
                    'optimizer_state_dict': optimizer.state_dict(),  # ì˜µí‹°ë§ˆì´ì € ì €ì¥
                    'loss': loss.item(),  # Loss ì €ì¥
                    'train_step': i,  # í˜„ì¬ ì§„í–‰í•œ í•™ìŠµ
                    'total_train_step': len(train_loader)  # í˜„ì¬ epochì— í•™ìŠµ í•  ì´ train step
                }, save_ckpt_path)
    return np.mean(losses)
```
- ë°˜ë³µë¬¸ ì‹¤í–‰: DataLoaderë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤. ê° ë°˜ë³µì—ì„œ, ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  ì¶œë ¥ì„ ì–»ìŠµë‹ˆë‹¤.

- ì†ì‹¤ ê³„ì‚°: ëª¨ë¸ì˜ ì¶œë ¥ê³¼ ì‹¤ì œ ë ˆì´ë¸”ì„ ë¹„êµí•˜ì—¬ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì†ì‹¤ì€ ë°°ì¹˜ ë‚´ ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•œ í‰ê·  ì†ì‹¤ì…ë‹ˆë‹¤.

- ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸: ì†ì‹¤ì„ í†µí•´ ê³„ì‚°ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜µí‹°ë§ˆì´ì €ê°€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³ , ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•˜ë©°, ì˜µí‹°ë§ˆì´ì €ì˜ step() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

- ì²´í¬í¬ì¸íŠ¸ ì €ì¥: ì§€ì •ëœ ìŠ¤í…ë§ˆë‹¤ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‚˜ì¤‘ì— í•™ìŠµì„ ì¤‘ë‹¨í•œ ì§€ì ì—ì„œ ì´ì–´ì„œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- í‰ê·  ì†ì‹¤ ë°˜í™˜: í•¨ìˆ˜ê°€ ì™„ë£Œë˜ë©´, ì—í¬í¬ ë™ì•ˆ ë°œìƒí•œ ëª¨ë“  ì†ì‹¤ì˜ í‰ê· ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í•™ìŠµ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ì„±ëŠ¥ ê°œì„ ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


### :hourglass_flowing_sand: **path ì„¤ì •** 
```
    torch.cuda.empty_cache()
    root_path = "/home/jupyter-j8b101/kobert-wellness-chatbot"
    data_path = f"{root_path}/data/wellness_dialog_for_text_classification_all.txt"
    checkpoint_path = f"{root_path}/checkpoint"
    save_ckpt_path = f"{checkpoint_path}/kobert-wellness-text-classification.pth"
```
### ğŸ¾ **Hyper Parameter**
```python 
batch_size = 4  
ctx = "cuda:4"
device = torch.device(ctx)
save_step = 100  # í•™ìŠµ ì €ì¥ ì£¼ê¸°
learning_rate = 5e-6  # Learning Rate
```

### :lemon: **data load**
```python 
# WellnessTextClassificationDataset Data Loader
dataset = WellnessTextClassificationDataset(file_path=data_path, device=device)
train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
model = KoBERTforSequenceClassfication()
model.to(device)
```
### :house: **Prepare optimizer**
```python 
    # Prepare optimizer and schedule (linear warmup and decay)
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
         'weight_decay': 0.01},
        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
    ]
    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
    pre_epoch, pre_loss, train_step = 0, 0, 0
```
- optimizer_grouped_parameters: ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê°€ì¤‘ì¹˜ ê°ì†Œë¥¼ ì ìš©í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ê·¸ë£¹ì—ëŠ” ê°€ì¤‘ì¹˜ ê°ì†Œë¥¼ ì ìš©í•˜ê³ , ë‘ ë²ˆì§¸ ê·¸ë£¹ì—ëŠ” ê°€ì¤‘ì¹˜ ê°ì†Œë¥¼ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

- optimizer: ë‘ ê·¸ë£¹ì˜ íŒŒë¼ë¯¸í„°ì™€ í•™ìŠµë¥ ì„ ì‚¬ìš©í•˜ì—¬ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### ğŸ”« **ëª¨ë¸ í•™ìŠµ**

```python 
# ì €ì¥ ì½”ë“œ 
if os.path.isfile(save_ckpt_path):
    checkpoint = torch.load(save_ckpt_path, map_location=device)
    pre_epoch = checkpoint['epoch']
    train_step = checkpoint['train_step']
    total_train_step = checkpoint['total_train_step']

# loss ì‹œê° í™”ë¥¼ ìœ„í•œ ì½”ë“œ 
losses = []
X = []
Y = []
offset = pre_epoch
for step in range(n_epoch):
    epoch = step + offset
    loss = train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)
    print(f"loss : {loss}")
    X.append(step)
    Y.append(loss)
plt.plot(X,Y)
plt.title('Training Loss')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.show()
```


### ğŸ“¤ **test** 

```python
import torch
import torch.nn as nn
import random

from model.classifier import KoBERTforSequenceClassfication
from kobert_transformers import get_tokenizer


def load_wellness_answer():
    root_path = "./kobert-wellness-chatbot"
    category_path = f"{root_path}/data/wellness_dialog_category.txt"
    answer_path = f"{root_path}/data/wellness_dialog_answer.txt"

    c_f = open(category_path, 'r')
    a_f = open(answer_path, 'r')

    category_lines = c_f.readlines()
    answer_lines = a_f.readlines()

    category = {}
    answer = {}
    for line_num, line_data in enumerate(category_lines):
        data = line_data.split('    ')
        category[data[1][:-1]] = data[0]

    for line_num, line_data in enumerate(answer_lines):
        data = line_data.split('    ')
        keys = answer.keys()
        if (data[0] in keys):
            answer[data[0]] += [data[1][:-1]]
        else:
            answer[data[0]] = [data[1][:-1]]

    return category, answer


def kobert_input(tokenizer, str, device=None, max_seq_len=512):
    index_of_words = tokenizer.encode(str)
    token_type_ids = [0] * len(index_of_words)
    attention_mask = [1] * len(index_of_words)

    # Padding Length
    padding_length = max_seq_len - len(index_of_words)

    # Zero Padding
    index_of_words += [0] * padding_length
    token_type_ids += [0] * padding_length
    attention_mask += [0] * padding_length

    data = {
        'input_ids': torch.tensor([index_of_words]).to(device),
        'token_type_ids': torch.tensor([token_type_ids]).to(device),
        'attention_mask': torch.tensor([attention_mask]).to(device),
    }
    return data


if __name__ == "__main__":
    root_path = "./kobert-wellness-chatbot"
    checkpoint_path = f"{root_path}/checkpoint"
    save_ckpt_path = f"{checkpoint_path}/kobert-wellness-text-classification.pth"


    category, answer = load_wellness_answer()

    ctx = "cuda:7" if torch.cuda.is_available() else "cpu"
    device = torch.device(ctx)

    checkpoint = torch.load(save_ckpt_path, map_location=device)

    model = KoBERTforSequenceClassfication()
    model.load_state_dict(checkpoint['model_state_dict'])

    model.to(ctx)
    model.eval()

    tokenizer = get_tokenizer()

    while 1:
        sent = input('\nQuestion: ')  
        data = kobert_input(tokenizer, sent, device, 512)

        if 'ì¢…ë£Œ' in sent:
            break
        output = model(**data)
        logit = output[0]
        softmax_logit = torch.softmax(logit, dim=-1)
        softmax_logit = softmax_logit.squeeze()

        max_index = torch.argmax(softmax_logit).item()
        max_index_value = softmax_logit[torch.argmax(softmax_logit)].item()

        answer_list = answer[category[str(max_index)]]
        answer_len = len(answer_list) - 1
        answer_index = random.randint(0, answer_len)
        print(f'Answer: {answer_list[answer_index]}, index: {max_index}, softmax_value: {max_index_value}')
        print('-' * 50)
```

### ğŸ“© **OUT**
```
input_text = "ìš”ìƒˆ ì¢€ ìš°ìš¸í•˜ê³  í˜ë“¤ë”ë¼ê³ "
kobert(answer) # "ì´í•´í•´ìš”. ì•„ë¬´ ì´ìœ  ì—†ì´ ìš°ìš¸í•  ë•Œê°€ ìˆì£ ."
```

### âœ¨ ê²°ê³¼ë¬¼
**Loss**
- CrossEntropyLoss() - êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
- <img src = "https://user-images.githubusercontent.com/109534450/229016538-98df19eb-1bd3-4f78-8aff-abf498ca8759.png" width="35%" height="35%">
- ë‘Â í™•ë¥ Â ë¶„í¬ì˜Â ì°¨ì´ë¥¼Â êµ¬í•˜ê¸°Â ìœ„í•´ì„œÂ ì‚¬ìš©ë©ë‹ˆë‹¤.
- ì‹¤ì œÂ ë°ì´í„°ì˜Â í™•ë¥ Â ë¶„í¬ì™€, í•™ìŠµëœ ëª¨ë¸ì´Â ê³„ì‚°í•œ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.

<img src = "https://user-images.githubusercontent.com/109534450/229391092-b4e0e45c-8d9d-4ff4-b7b3-c1716b6b945f.png" width="55%" height="55%">


### **ğŸ”§ ê°œë°œ í™˜ê²½**
- Google Colab
- Jupyter Hub
