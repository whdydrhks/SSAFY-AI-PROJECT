# ğŸ—¿ AI ğŸ©º ì‚¬ìš© ëª¨ë¸ : KoBERT / KoGPT2
---

## ğŸ›¹ KoBERT ?

- Bert ëª¨ë¸ì˜ í•œêµ­ì–´ ë²„ì „ìœ¼ë¡œ, SKTì—ì„œ  êµ¬ê¸€ì˜ BERT ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤. 

- ìì—°ì–´ ì²˜ë¦¬(NLP)ê°€ ê°€ëŠ¥í•œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.

- í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹, ë¬¸ì¥ ìœ ì‚¬ë„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ë„¤ì´ë²„ì—ì„œ ê³µê°œí•œ í•œê¸€ ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„°ë¥¼ ì‚¬ì „ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.

- [KoBERT ê¹ƒí—ˆë¸Œ](https://github.com/SKTBrain/KoBERT)

--- 
## ğŸ›¶ BERT?
- BERTëŠ” "Bidirectional Encoder Representations from Transformers"ì˜ ì•½ì–´ë¡œ, êµ¬ê¸€ì—ì„œ ê°œë°œí•œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. 
- BERTì˜ ì¥ì ì¸ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë¬¸ì¥ ë¶„ë¥˜, ë¬¸ì¥ ìœ ì‚¬ë„, ì§ˆë¬¸ ì‘ë‹µ ë“±ì˜ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

- Transformerë¼ëŠ” ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 
    * TransformerëŠ” Attention mechanismì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ëœ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì˜ì¡´ ê´€ê³„ë¥¼ ì°¾ì•„ë‚´ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.
- [BERT ê¹ƒí—ˆë¸Œ](https://github.com/google-research/bert)

---

## ğŸ›« KoGPT2 ?

- KoGPT2ëŠ” SKTì—ì„œ ê°œë°œí•œ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ GPT ëª¨ë¸ì…ë‹ˆë‹¤. GPTëŠ” Generative Pre-trained Transformerì˜ ì•½ìë¡œ, íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±ì— ëŒ€í•œ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

- KoGPT2ëŠ” í•œêµ­ì–´ ë¬¸ì¥ì— ëŒ€í•œ í† í°í™”(tokenization), ì„ë² ë”©(embedding), GPT ëª¨ë¸ í•™ìŠµ ë° í…ìŠ¤íŠ¸ ìƒì„±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ëª¨ë“ˆë“¤ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ì–‘í•œ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- KoGPT2ë¥¼ ì´ìš©í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´, ë¨¼ì € ë¬¸ì¥ì„ í† í°í™”í•˜ê³  KoGPT ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ì˜ˆì¸¡ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´í›„, ì–»ì€ ì˜ˆì¸¡ê°’ì„ ì´ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- [KoGPT2 ê¹ƒí—ˆë¸Œ](https://github.com/SKT-AI/KoGPT2)
---

## â›µ GPT2
- GPT-2 (Generative Pre-trained Transformer 2)ëŠ” OpenAIì—ì„œ ê°œë°œí•œ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì…ë‹ˆë‹¤. 
- GPT-2ëŠ” Transformer ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ë©°, í…ìŠ¤íŠ¸ ìƒì„±, ê¸°ê³„ ë²ˆì—­, ì§ˆì˜ ì‘ë‹µ, ìš”ì•½, ê°ì„± ë¶„ì„ ë“± ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ëŒ€í•´ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

- [GPT-2 ê¹ƒí—ˆë¸Œ](https://github.com/openai/gpt-2)

---

## ğŸ’¡ **ê°ì • ë¶„ì„ ğŸ‘‰ KoBERT**

### âœï¸ **Process**

1. **ì…ë ¥ ë°ì´í„° ì „ ì²˜ë¦¬**
    - ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ í† í° IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë¬¸ì¥ì˜ ì‹¤ì œ ê¸¸ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ë¬¸ì¥ ìŒì˜ ê²½ìš°, ë¬¸ì¥ ê°„ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” ì„¸ê·¸ë¨¼íŠ¸ IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. **BERT ëª¨ë¸ ì ìš©**
    - ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°ì´í„°ë¥¼ BERT ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì¶œë ¥ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
    - BERT ëª¨ë¸ì€ ì…ë ¥ í† í°ì„ ì„ë² ë”©í•˜ê³ , ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê° í† í°ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    - ì¶œë ¥ê°’ì€ BERTì˜ ì¶œë ¥ ì°¨ì› í¬ê¸°ì¸ 768ì°¨ì›ìœ¼ë¡œ ë‚˜ì˜µë‹ˆë‹¤.
3. **ì¶œë ¥ê°’ ë³€í™˜**
    - BERT ëª¨ë¸ì˜ ì¶œë ¥ê°’ ì¤‘ ë¬¸ì¥ì„ ëŒ€í‘œí•˜ëŠ” poolerë¥¼ ì‚¬ìš©í•˜ì—¬ 768ì°¨ì› ë²¡í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    - ë²¡í„°ë¥¼ ì„ í˜• ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ë ˆì´ë¸”ì— ëŒ€í•œ ë¡œì§“ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ë¡œì§“ì€ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
4. **ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™”**
    - logitê³¼ ì‹¤ì œ í´ë˜ìŠ¤ ë ˆì´ë¸” ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
    - í•™ìŠµì—ëŠ” í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent)ì˜ ë³€í˜• ì¤‘ í•˜ë‚˜ì¸ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - í•™ìŠµë¥ ê³¼ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ë“±ì„ ì¡°ì •í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.

### ğŸ§¾ Dataset
![ê°ì •ë¶„ì„](https://user-images.githubusercontent.com/109534450/229683701-ceb7d12e-81bc-478a-8efb-519c14c7ad9c.PNG)
- [AI Hub ê°ì„± ëŒ€í™” ë§ë­‰ì¹˜ ë°ì´í„° ì…‹](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=86)
- ì¶”ê°€ ë°ì´í„° ìƒì„± ë° í•™ìŠµ

### ğŸ›  Requirements

```
!pip install gluonnlp pandas tqdm
!pip install mxnet
!pip install sentencepiece
!pip install transformers
!pip install torch
!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'

```

### âš¾ï¸ Import

```
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook
import pandas as pd
from sklearn.model_selection import train_test_split
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

from google.colab import drive
drive.mount('/content/drive')

```

- **torch**
- **nn, optim, F, Dataset, DataLoader :** íŒŒì´í† ì¹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í•˜ìœ„ ëª¨ë“ˆ, ì‹ ê²½ë§ ëª¨ë¸, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í•¨ìˆ˜, ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë”
- **gluonnlp :** MXNet í”„ë ˆì„ ì›Œí¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **numpy :** ë‹¤ì°¨ì› í–‰ë ¬ ê´€ë ¨
- **pandas :** ë°ì´í„° ì²˜ë¦¬
- **KoBERTTokenizer :** KoBERTì˜ í† í¬ë‚˜ì´ì €
- **BertModel**
- **AdamW :** AdamW ìµœì í™”
- **get_cosine_schedule_with_warmup :** learning rate ìŠ¤ì¼€ì¤„ë§ ì ìš© í•¨ìˆ˜
- **drive.mount('/content/drive/') :** êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ Colabì— ë™ê¸°í™”
- **device = torch.device("cuda:0") :** GPU ì‚¬ìš©

### ğŸ¾ Hyper Parameter

```
# Setting parameters
max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 50
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5
```

***í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì…‹íŒ…***

- **max_len** ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´
- **batch_size** í•œ ë²ˆì˜ batchë§ˆë‹¤ ì£¼ëŠ” ë°ì´í„° ìƒ˜í”Œì˜ size. 
- **num_epochs :** ì „ì²´ í•™ìŠµ ë°ì´í„° í•™ìŠµ íšŸìˆ˜
- **learning_rate :** í•™ìŠµë¥ 

### ğŸ§šâ€â™€ï¸ BERT Dataset

```
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,
                 pad, pair):

        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))
```

- gluonnlpì˜ BERTSentenceTransformì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë¬¸ì¥ì„ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
-transform ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì¥ì„ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜

### ğŸ§šâ€â™‚ï¸ BERT Classifier

```
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=6,
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate

        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)

    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)

        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)

```

- ì…ë ¥ ë°ì´í„°ì˜ íŒ¨ë”© ë¶€ë¶„ì„ ì œì™¸í•˜ê³ , ì‹¤ì œ ì…ë ¥ì— ëŒ€í•œ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
- token_idsëŠ” ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•œ ê²°ê³¼
- gen_attention_mask ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±
- BERT ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ì—¬ ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

### ğŸ”« í† í¬ ë‚˜ì´ì € / ëª¨ë¸ ì •ì˜

```
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')
tok = tokenizer.tokenize
```

- skt/kobert-base-v1 ëª¨ë¸ì˜ **ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜** ë¡œë“œ
- BERTVocab ê°ì²´ì— tokenizer.vocab_file ì‚¬ì „(vocab) ë¡œë“œ

### ğŸ’¿ ë°ì´í„° ë¡œë“œ/ ì „ì²˜ë¦¬

```
train_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…¡á†·á„Œá…¥á†¼á„‡á…®á†«á„‰á…¥á†¨dataset.csv', encoding='cp949')
validation_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Validation.csv',encoding='cp949')
train_set = train_set.loc[:, ['sentiment', 'user']]
validation_set = validation_set.loc[:, ['ê°ì •_ëŒ€ë¶„ë¥˜', 'ì‚¬ëŒë¬¸ì¥1']]

train_set.dropna(inplace=True)
validation_set.dropna(inplace=True)
train_set.columns = ['label', 'data']
validation_set.columns = ['label', 'data']

train_set.loc[(train_set['label'] == 'ì¼ìƒ'), 'label'] = 0
train_set.loc[(train_set['label'] == 'ë¶„ë…¸'), 'label'] = 1
train_set.loc[(train_set['label'] == 'ë¶ˆì•ˆ'), 'label'] = 2
train_set.loc[(train_set['label'] == 'ìŠ¬í””'), 'label'] = 3
train_set.loc[(train_set['label'] == 'ê¸°ì¨'), 'label'] = 4
train_set.loc[(train_set['label'] == 'ìš°ìš¸'), 'label'] = 5

validation_set.loc[(validation_set['label'] == 'ì¼ìƒ'), 'label'] = 0
validation_set.loc[(validation_set['label'] == 'ë¶„ë…¸'), 'label'] = 1
validation_set.loc[(validation_set['label'] == 'ë¶ˆì•ˆ'), 'label'] = 2
validation_set.loc[(validation_set['label'] == 'ìŠ¬í””'), 'label'] = 3
validation_set.loc[(validation_set['label'] == 'ê¸°ì¨'), 'label'] = 4
validation_set.loc[(validation_set['label'] == 'ìš°ìš¸'), 'label'] = 5

train_set_data = [[i, str(j)] for i, j in zip(train_set['data'], train_set['label'])]

# validation_set_data = [[i, str(j)] for i, j in zip(validation_set['data'], validation_set['label'])]

train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=4)
train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)
test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)
train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=2)
test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=2)

```

### ğŸ§¸ ëª¨ë¸ í•™ìŠµ

```
model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)
# Prepare optimizer and schedule (linear warmup and decay)
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()
t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)

for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()  # Update learning rate schedule
        train_acc += calc_accuracy(out, label)
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
    print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
    model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        test_acc += calc_accuracy(out, label)
    print("epoch {} test acc {}".format(e+1, test_acc / (batch_id+1)))

```

**train ê³¼ validation ì§„í–‰**

### **í•™ìŠµ ì„¤ì •ê°’**

- BERTClassifier í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ëª¨ë¸ì€ BERT ëª¨ë¸ê³¼ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ë¡œ êµ¬ì„±
- AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
- ê°€ì¤‘ì¹˜ ê°ì‡ (weight decay)ë¥¼ ì ìš©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
- ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì ìš©í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ì™€ ì ìš©í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ë¶„í•˜ì—¬ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •
- CrossEntropyLoss í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

### **í•™ìŠµ ë™ì‘ê³¼ì • (for ~)**

- ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
- token_ids, segment_ids, valid_length, label ê°’ì„ GPU ì˜¬ë¦¬ê³ , modelì— input -> out
- out ê°’ê³¼ label ê°’ì„ ì‚¬ìš©í•˜ì—¬ Cross Entropy Lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
- loss.backward() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ëª¨ë¸ì˜ ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°
- optimizer.step() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ ê°’ì„ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
- CosineAnnealingWarmRestarts ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©, í•™ìŠµë¥ ì„ ì¡°ì •
- í˜„ì¬ ë°°ì¹˜ì˜ í•™ìŠµ ì •í™•ë„ train_accì— ëˆ„ì 
- í˜„ì¬ epochì˜ í•™ìŠµ ì •í™•ë„ë¥¼ ì¶œë ¥

### **ğŸ§¬ ê°ì •**
**ğŸ˜€ì¼ìƒ - 0 ğŸ˜Šê¸°ì¨ -1 ğŸ˜§ë¶ˆì•ˆ - 2 ğŸ˜­ìŠ¬í”” - 3 ğŸ˜¡ë¶„ë…¸ - 4 ğŸ˜¥ìš°ìš¸ - 5**

<img src = "https://user-images.githubusercontent.com/109534450/229018209-f9fb7af0-0800-47f8-981b-3ea3e13d2e9d.png" width="55%" height="55%">


### ğŸ“¤ Predict

```
def calc_accuracy(X,Y):
    max_vals, max_indices = torch.max(X, 1)
    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
    return train_acc
def predict(sentence):
    dataset = [[sentence, '0']]
    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)
    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=2)
    model.eval()
    answer = 0
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        for logits in out:
            logits = logits.detach().cpu().numpy()
            answer = np.argmax(logits)
    return answer
```

- predict í•¨ìˆ˜ëŠ” ì…ë ¥ ë¬¸ì¥ì„ ì…ë ¥ ë°›ì•„ í•´ë‹¹ ë¬¸ì¥ ê°ì • ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
- ì…ë ¥ ë¬¸ì¥ì„ datasetì— ì¶”ê°€í•˜ì—¬ BERTDataset ê°ì²´ë¥¼ ìƒì„±
- ëª¨ë¸ í‰ê°€ ëª¨ë“œ(model.eval())
- DataLoaderì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
- ëª¨ë¸ ì¶œë ¥ ê°’ì—ì„œ ê°€ì¥ í° ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ ê°’ìœ¼ë¡œ ë°˜í™˜

### ğŸ“© **OUT**
<img src = "https://user-images.githubusercontent.com/109534450/229065068-57d1e4f1-b9f1-461c-a0d2-d8a1d966d20f.png" width="55%" height="55%">

### âœ¨ ê²°ê³¼ë¬¼
**Loss**
- CrossEntropyLoss() - êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
- <img src = "https://user-images.githubusercontent.com/109534450/229016538-98df19eb-1bd3-4f78-8aff-abf498ca8759.png" width="35%" height="35%">
- ë‘Â í™•ë¥ Â ë¶„í¬ì˜Â ì°¨ì´ë¥¼Â êµ¬í•˜ê¸°Â ìœ„í•´ì„œÂ ì‚¬ìš©ë©ë‹ˆë‹¤.
- ì‹¤ì œÂ ë°ì´í„°ì˜Â í™•ë¥ Â ë¶„í¬ì™€, í•™ìŠµëœ ëª¨ë¸ì´Â ê³„ì‚°í•œ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.

<img src = "https://user-images.githubusercontent.com/109534450/229390889-8b9a2114-675f-4529-aacf-29a2fbd19055.png" width="55%" height="55%">



**Accuracy**


<img src = "https://user-images.githubusercontent.com/109534450/229020020-d61be612-8721-4348-9961-390a18955766.png" width="55%" height="55%">


**Confusion Matrix**


<img src = "https://user-images.githubusercontent.com/109534450/229709741-e4cbb9bd-9d79-4d3c-8180-ae4e9e5bed35.png" width="55%" height="55%">


---

---
## ğŸ’¡ **ê°ì„± ì±—ë´‡ ğŸ‘‰ KoGPT2**
### âœï¸ **Process**

1. ë°ì´í„° ì „ì²˜ë¦¬

- ëŒ€í™” ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ í† í° IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
ì§ˆë¬¸ê³¼ ëŒ€ë‹µ ê°„ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
ë¬¸ì¥ ìŒì˜ ê²½ìš°, ë¬¸ì¥ ê°„ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°, ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ ì˜ë¼ëƒ…ë‹ˆë‹¤.
2. koGPT ëª¨ë¸ ì ìš©

- ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°ì´í„°ë¥¼ KoGPT2 ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì¶œë ¥ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
KoGPT2 ëª¨ë¸ì€ ì…ë ¥ í† í°ì„ ì„ë² ë”©í•˜ê³ , ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê° í† í°ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
ì¶œë ¥ê°’ì€ KoGPT2 ëª¨ë¸ì˜ ì¶œë ¥ ì°¨ì› í¬ê¸°ì™€ ë™ì¼í•œ í¬ê¸°ì˜ ë¡œì§“ìœ¼ë¡œ ë‚˜ì˜µë‹ˆë‹¤.
3. ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™”

- logitê³¼ ì‹¤ì œ ë ˆì´ë¸” ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
í•™ìŠµì—ëŠ” Adam ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
í•™ìŠµë¥ ê³¼ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ë“±ì„ ì¡°ì •í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
4. Predict

- KoGPT2 ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì…ë ¥ëœ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
ì¸ì½”ë”©ëœ input_idsë¥¼ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í°ì„ ì„ íƒí•˜ì—¬ ëŒ€ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

### ğŸ§¾ **Dataset**
![bert_ì±—ë´‡](https://user-images.githubusercontent.com/109534450/229683525-159091de-309a-417f-8749-5754cffc37a1.PNG)
- [ê°ì„± ëŒ€í™” ë§ë­‰ì¹˜ ë°ì´í„° ì…‹ (AI Hub)](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=86)
- ì¶”ê°€ ë°ì´í„° ìƒì„± ë° í•™ìŠµ

### ğŸ›  **Requirements**
```
! pip install transformers
! pip install pytorch-lightning
! pip install torch
```

### âš¾ï¸ **Import**

```
import numpy as np
import pandas as pd
import torch
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning import LightningModule
from torch.utils.data import DataLoader, Dataset
from transformers.optimization import AdamW, get_cosine_schedule_with_warmup
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
import re

```
### **ğŸ¥Œ í† í°**
```python
Q_TKN = "<usr>"
A_TKN = "<sys>"
BOS = '</s>'
EOS = '</s>'
MASK = '<unused0>'
SENT = '<unused1>'
PAD = '<pad>
```
- Q_TKN = "<usr>" : ì§ˆë¬¸ ìœ ì € í† í°
- A_TKN = "<sys>" : ëŒ€ë‹µ ì‹œìŠ¤í…œ í† í°
- BOS = '</s>' : ë¬¸ì¥ì˜ ì‹œì‘
- EOS = '</s>' : ë¬¸ì¥ì˜ ëì„
- MASK = '<unused0>' : ë§ˆìŠ¤í¬ ì²˜ë¦¬
- SENT = '<unused1>' : ë¬¸ì¥ ì²˜ë¦¬
- PAD = '<pad>' : íŒ¨ë”© 

### ğŸ¾ **Hyper Parameter**

```
# Setting parameters
learning_rate = 3e-5
criterion = torch.nn.CrossEntropyLoss(reduction="mean")
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
epoch = 70
Sneg = -1e18
```

***í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì…‹íŒ…***
- criterion :  CrossEntropyLoss 
- optimizer : Adam 

### ğŸ§šâ€â™€ï¸ **KoGPT2 Chatbot Dataset**

```python 
class ChatbotDataset(Dataset):
    def __init__(self, chats, max_len=100):  # ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ë¶€ë¶„
        self._data = chats
        self.max_len = max_len
        self.q_token = Q_TKN
        self.a_token = A_TKN
        self.sent_token = SENT
        self.eos = EOS
        self.mask = MASK
        self.tokenizer = koGPT2_TOKENIZER

    def __len__(self):  # chatbotdata ì˜ ê¸¸ì´ë¥¼ ë¦¬í„´í•œë‹¤.
        return len(self._data)

    def __getitem__(self, idx):  # ë¡œë“œí•œ ì±—ë´‡ ë°ì´í„°ë¥¼ ì°¨ë¡€ì°¨ë¡€ DataLoaderë¡œ ë„˜ê²¨ì£¼ëŠ” ë©”ì„œë“œ
        turn = self._data.iloc[idx]
        q = turn["ì‚¬ëŒë¬¸ì¥1"]  # ì§ˆë¬¸ì„ ê°€ì ¸ì˜¨ë‹¤.
        q = re.sub(r"([?.!,])", r" ", q)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.

        a = turn["ì‹œìŠ¤í…œë¬¸ì¥1"]  # ë‹µë³€ì„ ê°€ì ¸ì˜¨ë‹¤.
        a = re.sub(r"([?.!,])", r" ", a)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.

        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)
        q_len = len(q_toked)

        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)
        a_len = len(a_toked)

        #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´
        if q_len > self.max_len:
            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´
                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ 
                q_len = len(q_toked)
                a_len = self.max_len - q_len              #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            a_toked = a_toked[:a_len]
            a_len = len(a_toked)

        #ì§ˆë¬¸ì˜ ê¸¸ì´ + ë‹µë³€ì˜ ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´
        if q_len + a_len > self.max_len:
            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´
                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ 
                q_len = len(q_toked)
                a_len = self.max_len - q_len              #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            a_toked = a_toked[:a_len]
            a_len = len(a_toked)

        # ë‹µë³€ labels = [mask, mask, ...., mask, ..., <bos>,..ë‹µë³€.. <eos>, <pad>....]
        labels = [self.mask,] * q_len + a_toked[1:]

        # mask = ì§ˆë¬¸ê¸¸ì´ 0 + ë‹µë³€ê¸¸ì´ 1 + ë‚˜ë¨¸ì§€ 0
        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)
        # ë‹µë³€ labelsì„ index ë¡œ ë§Œë“ ë‹¤.
        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)
        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING
        while len(labels_ids) < self.max_len:
            labels_ids += [self.tokenizer.pad_token_id]

        # ì§ˆë¬¸ + ë‹µë³€ì„ index ë¡œ ë§Œë“ ë‹¤.    
        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)
        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING
        while len(token_ids) < self.max_len:
            token_ids += [self.tokenizer.pad_token_id]

        #ì§ˆë¬¸+ë‹µë³€, ë§ˆìŠ¤í¬, ë‹µë³€
        return (token_ids, np.array(mask), labels_ids)

def collate_batch(batch):
    data = [item[0] for item in batch]
    mask = [item[1] for item in batch]
    label = [item[2] for item in batch]
    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)
```


### ğŸ”« **í† í¬ ë‚˜ì´ì € / ëª¨ë¸ ì •ì˜**

```python
koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
            bos_token=BOS, eos_token=EOS, unk_token='<unk>',
            pad_token=PAD, mask_token=MASK) 
model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
```

- skt/kogpt2-base-v2 ëª¨ë¸ì— ëŒ€í•œ í† í¬ë‚˜ì´ì € ë¡œë“œ
- GPT2LMHeadModelì€ GPT-2 ëª¨ë¸ ë¡œë“œ, 'skt/kogpt2-base-v2' ëª¨ë¸

### ğŸ’¿ **ë°ì´í„° ë¡œë“œ/ ì „ì²˜ë¦¬**

```python
import urllib.request

Chatbot_Data = pd.read_csv("./data/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.csv", encoding="cp949")
Chatbot_Data = Chatbot_Data[:51629]
Chatbot_Data.head()
train_set = ChatbotDataset(Chatbot_Data, max_len=100)
train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)
```

### ğŸ§¸ **ëª¨ë¸ í•™ìŠµ**

```python
import matplotlib.pyplot as plt

losses = []
X = []
Y = []
print("start")
for epoch in range(epoch):
    print(epoch)
    X.append(epoch)
    for batch_idx, samples in enumerate(train_dataloader):
        optimizer.zero_grad()
        token_ids, mask, label = samples
        token_ids = token_ids.to(device)
        mask = mask.to(device)
        label = label.to(device)
        out = model(token_ids)        
        out = out.logits.to(device)
        
        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2).to(device)
        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out)).to(device)
        loss = criterion(mask_out.transpose(2, 1), label).to(device)
        
        avg_loss = loss.sum() / mask.sum()
        avg_loss.backward()
        optimizer.step()
    Y.append(loss.data.cpu().numpy())
print("end")

```
- train_dataloaderì—ì„œ token_ids, mask, label ê°’ì„ í• ë‹¹

- optimizerë¥¼ ì´ˆê¸°í™”í•˜ê³ , ëª¨ë¸ì˜ ì¶œë ¥ ê°’ outì„ ê³„ì‚°, logits ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ ë¡œì§“ ê°’ì„ ê³„ì‚°

- mask ê°’ì„ 3ì°¨ì›ìœ¼ë¡œ í™•ì¥, outê³¼ ë™ì¼í•œ í¬ê¸°ë¡œ ë°˜ë³µí•˜ì—¬ mask_3dë¥¼ ìƒì„±.
- torch.where ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ maskê°€ 1ì¸ ìœ„ì¹˜ëŠ” out ê°’ì„, 0ì¸ ìœ„ì¹˜ëŠ” Negative infinity ê°’ì„ ê°€ì§€ëŠ” í…ì„œë¥¼ ìƒì„±í•˜ì—¬ mask_out ë³€ìˆ˜ì— í• ë‹¹

- ì†ì‹¤ í•¨ìˆ˜(criterion)ë¥¼ ê³„ì‚°í•˜ê³ , loss ê°’ì„ ì´ìš©í•˜ì—¬ í˜„ì¬ ì†ì‹¤ ê°’ì„ êµ¬í•©ë‹ˆë‹¤. ì´ë•Œ, ì†ì‹¤ ê°’ì˜ í‰ê· ì„ êµ¬í•˜ê¸° ìœ„í•´ avg_loss ë³€ìˆ˜ë¥¼ ê³„ì‚°

- backward ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê³ , step ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸

### ğŸ“¤ **Predict**

```python
def kogpt(input_text):
    q = input_text
    a = ""
    sent = ""
    while True:
        input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + sent + A_TKN + a)).unsqueeze(dim=0)
        pred = model(input_ids)
        pred = pred.logits
        gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().tolist())[-1]
        if gen == EOS:
            break
        a += gen.replace("â–", " ")
    return a
```

-  koGPT2 ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì…ë ¥ëœ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
- ì¸ì½”ë”©ëœ input_idsë¥¼ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ pred ë³€ìˆ˜ì— ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥
- torch.argmax í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í°ì„ ì„ íƒí•˜ê³ , ì´ë¥¼ gen ë³€ìˆ˜ì— ì €ì¥, ì´ë•Œ convert_ids_to_tokens í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì„ íƒëœ í† í°ì„ ë¬¸ìì—´ë¡œ ë³€í™˜

- gen ë³€ìˆ˜ê°€ EOSì¸ ê²½ìš°, ìƒì„± ê³¼ì •ì„ ë§ˆì¹¨. ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° genì— ëŒ€í•œ ê°’ì„ aì— ì¶”ê°€

### ğŸ“© **OUT**

```
input_text = "ì˜¤ëŠ˜ ê°‘ìê¸° ë‚ ì”¨ê°€ ì¶”ì›Œì ¸ì„œ ë†€ëì–´"
kogpt(sentence) # "ë‚ ì”¨ê°€ ë§ì´ ì¶”ì›Œì¡Œêµ°ìš”"
```



## ğŸ’¡ **ê°ì„± ì±—ë´‡ ğŸ‘‰ KoBERT**



### ğŸ§¾ **ë°ì´í„° ì…‹**
![bert_ì±—ë´‡ëŒ€í™”](https://user-images.githubusercontent.com/109534450/229684303-4779a711-26b8-41ac-bcb2-cc60e2470555.PNG)
- AIÂ HubÂ ì œê³µ,Â ì›°ë‹ˆìŠ¤Â ëŒ€í™”Â ìŠ¤í¬ë¦½íŠ¸Â ë°ì´í„°ì…‹ (_í˜„ì¬ í˜ì´ì§€ ì—†ìŒ_)

### ğŸ“© **OUT**
```
input_text = "ìš”ìƒˆ ì¢€ ìš°ìš¸í•˜ê³  í˜ë“¤ë”ë¼ê³ "
kobert(answer) # "ì´í•´í•´ìš”. ì•„ë¬´ ì´ìœ  ì—†ì´ ìš°ìš¸í•  ë•Œê°€ ìˆì£ ."
```



### âœ¨ ê²°ê³¼ë¬¼
**Loss**
- CrossEntropyLoss() - êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
- <img src = "https://user-images.githubusercontent.com/109534450/229016538-98df19eb-1bd3-4f78-8aff-abf498ca8759.png" width="35%" height="35%">
- ë‘Â í™•ë¥ Â ë¶„í¬ì˜Â ì°¨ì´ë¥¼Â êµ¬í•˜ê¸°Â ìœ„í•´ì„œÂ ì‚¬ìš©ë©ë‹ˆë‹¤.
- ì‹¤ì œÂ ë°ì´í„°ì˜Â í™•ë¥ Â ë¶„í¬ì™€, í•™ìŠµëœ ëª¨ë¸ì´Â ê³„ì‚°í•œ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.

<img src = "https://user-images.githubusercontent.com/109534450/229391092-b4e0e45c-8d9d-4ff4-b7b3-c1716b6b945f.png" width="55%" height="55%">


### **ğŸ”§ ê°œë°œ í™˜ê²½**
- Google Colab
- Jupyter Hub
