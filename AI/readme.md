# ğŸ©º ì‚¬ìš© ëª¨ë¸ : KoBERT / KoGPT2
---

## ğŸ›¹ KoBERT ?

- Bert ëª¨ë¸ì˜ í•œêµ­ì–´ ë²„ì „ìœ¼ë¡œ, SKTì—ì„œ  êµ¬ê¸€ì˜ BERT ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤. 

- ìì—°ì–´ ì²˜ë¦¬(NLP)ê°€ ê°€ëŠ¥í•œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.

- í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹, ë¬¸ì¥ ìœ ì‚¬ë„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ë„¤ì´ë²„ì—ì„œ ê³µê°œí•œ í•œê¸€ ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„°ë¥¼ ì‚¬ì „ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.

- [KoBERT ê¹ƒí—ˆë¸Œ](https://github.com/SKTBrain/KoBERT)

--- 
## ğŸ›¶ BERT?
- BERTëŠ” "Bidirectional Encoder Representations from Transformers"ì˜ ì•½ì–´ë¡œ, êµ¬ê¸€ì—ì„œ ê°œë°œí•œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. 
- BERTì˜ ì¥ì ì¸ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë¬¸ì¥ ë¶„ë¥˜, ë¬¸ì¥ ìœ ì‚¬ë„, ì§ˆë¬¸ ì‘ë‹µ ë“±ì˜ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

- Transformerë¼ëŠ” ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 
    * TransformerëŠ” Attention mechanismì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ëœ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì˜ì¡´ ê´€ê³„ë¥¼ ì°¾ì•„ë‚´ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.
- [BERT ê¹ƒí—ˆë¸Œ](https://github.com/google-research/bert)

---

## ğŸ›« KoGPT2 ?

~~ ì„¤ëª… ~~

---
## ğŸ’¡ **ê°ì • ë¶„ì„ ğŸ‘‰ KoBERT**

### âœï¸ **Process**

1. **ì…ë ¥ ë°ì´í„° ì „ ì²˜ë¦¬**
    - ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ í† í° IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë¬¸ì¥ì˜ ì‹¤ì œ ê¸¸ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ë¬¸ì¥ ìŒì˜ ê²½ìš°, ë¬¸ì¥ ê°„ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” ì„¸ê·¸ë¨¼íŠ¸ IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. **BERT ëª¨ë¸ ì ìš©**
    - ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°ì´í„°ë¥¼ BERT ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì¶œë ¥ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
    - BERT ëª¨ë¸ì€ ì…ë ¥ í† í°ì„ ì„ë² ë”©í•˜ê³ , ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê° í† í°ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    - ì¶œë ¥ê°’ì€ BERTì˜ ì¶œë ¥ ì°¨ì› í¬ê¸°ì¸ 768ì°¨ì›ìœ¼ë¡œ ë‚˜ì˜µë‹ˆë‹¤.
3. **ì¶œë ¥ê°’ ë³€í™˜**
    - BERT ëª¨ë¸ì˜ ì¶œë ¥ê°’ ì¤‘ ë¬¸ì¥ì„ ëŒ€í‘œí•˜ëŠ” poolerë¥¼ ì‚¬ìš©í•˜ì—¬ 768ì°¨ì› ë²¡í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    - ë²¡í„°ë¥¼ ì„ í˜• ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ë ˆì´ë¸”ì— ëŒ€í•œ ë¡œì§“ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ë¡œì§“ì€ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
4. **ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™”**
    - logitê³¼ ì‹¤ì œ í´ë˜ìŠ¤ ë ˆì´ë¸” ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
    - í•™ìŠµì—ëŠ” í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent)ì˜ ë³€í˜• ì¤‘ í•˜ë‚˜ì¸ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - í•™ìŠµë¥ ê³¼ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ë“±ì„ ì¡°ì •í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.

### ğŸ§¾ Dataset
- [AI Hub ê°ì„± ëŒ€í™” ë§ë­‰ì¹˜ ë°ì´í„° ì…‹](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=86)
- ì¶”ê°€ ë°ì´í„° ìƒì„± ë° í•™ìŠµ

### ğŸ›  Requirements

```
!pip install gluonnlp pandas tqdm
!pip install mxnet
!pip install sentencepiece
!pip install transformers
!pip install torch
!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'

```

### âš¾ï¸ Import

```
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook
import pandas as pd
from sklearn.model_selection import train_test_split
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

from google.colab import drive
drive.mount('/content/drive')

```

- **torch**
- **nn, optim, F, Dataset, DataLoader :** íŒŒì´í† ì¹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í•˜ìœ„ ëª¨ë“ˆ, ì‹ ê²½ë§ ëª¨ë¸, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í•¨ìˆ˜, ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë”
- **gluonnlp :** MXNet í”„ë ˆì„ ì›Œí¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **numpy :** ë‹¤ì°¨ì› í–‰ë ¬ ê´€ë ¨
- **pandas :** ë°ì´í„° ì²˜ë¦¬
- **KoBERTTokenizer :** KoBERTì˜ í† í¬ë‚˜ì´ì €
- **BertModel**
- **AdamW :** AdamW ìµœì í™”
- **get_cosine_schedule_with_warmup :** learning rate ìŠ¤ì¼€ì¤„ë§ ì ìš© í•¨ìˆ˜
- **drive.mount('/content/drive/') :** êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ Colabì— ë™ê¸°í™”
- **device = torch.device("cuda:0") :** GPU ì‚¬ìš©

### ğŸ¾ Hyper Parameter

```
# Setting parameters
max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 50
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5
```

***í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì…‹íŒ…***

- **max_len** ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´
- **batch_size** í•œ ë²ˆì˜ batchë§ˆë‹¤ ì£¼ëŠ” ë°ì´í„° ìƒ˜í”Œì˜ size. 
- **num_epochs :** ì „ì²´ í•™ìŠµ ë°ì´í„° í•™ìŠµ íšŸìˆ˜
- **learning_rate :** í•™ìŠµë¥ 

### ğŸ§šâ€â™€ï¸ BERT Dataset

```
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,
                 pad, pair):

        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))
```

- gluonnlpì˜ BERTSentenceTransformì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë¬¸ì¥ì„ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
-transform ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì¥ì„ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜

### ğŸ§šâ€â™‚ï¸ BERT Classifier

```
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=6,
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate

        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)

    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)

        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)

```

- ì…ë ¥ ë°ì´í„°ì˜ íŒ¨ë”© ë¶€ë¶„ì„ ì œì™¸í•˜ê³ , ì‹¤ì œ ì…ë ¥ì— ëŒ€í•œ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
- token_idsëŠ” ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•œ ê²°ê³¼
- gen_attention_mask ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±
- BERT ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ì—¬ ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

### ğŸ”« í† í¬ ë‚˜ì´ì € / ëª¨ë¸ ì •ì˜

```
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')
tok = tokenizer.tokenize
```

- skt/kobert-base-v1 ëª¨ë¸ì˜ **ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜** ë¡œë“œ
- BERTVocab ê°ì²´ì— tokenizer.vocab_file ì‚¬ì „(vocab) ë¡œë“œ

### ğŸ’¿ ë°ì´í„° ë¡œë“œ/ ì „ì²˜ë¦¬

```
train_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…¡á†·á„Œá…¥á†¼á„‡á…®á†«á„‰á…¥á†¨dataset.csv', encoding='cp949')
validation_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Validation.csv',encoding='cp949')
train_set = train_set.loc[:, ['sentiment', 'user']]
validation_set = validation_set.loc[:, ['ê°ì •_ëŒ€ë¶„ë¥˜', 'ì‚¬ëŒë¬¸ì¥1']]

train_set.dropna(inplace=True)
validation_set.dropna(inplace=True)
train_set.columns = ['label', 'data']
validation_set.columns = ['label', 'data']

train_set.loc[(train_set['label'] == 'ì¼ìƒ'), 'label'] = 0
train_set.loc[(train_set['label'] == 'ë¶„ë…¸'), 'label'] = 1
train_set.loc[(train_set['label'] == 'ë¶ˆì•ˆ'), 'label'] = 2
train_set.loc[(train_set['label'] == 'ìŠ¬í””'), 'label'] = 3
train_set.loc[(train_set['label'] == 'ê¸°ì¨'), 'label'] = 4
train_set.loc[(train_set['label'] == 'ìš°ìš¸'), 'label'] = 5

validation_set.loc[(validation_set['label'] == 'ì¼ìƒ'), 'label'] = 0
validation_set.loc[(validation_set['label'] == 'ë¶„ë…¸'), 'label'] = 1
validation_set.loc[(validation_set['label'] == 'ë¶ˆì•ˆ'), 'label'] = 2
validation_set.loc[(validation_set['label'] == 'ìŠ¬í””'), 'label'] = 3
validation_set.loc[(validation_set['label'] == 'ê¸°ì¨'), 'label'] = 4
validation_set.loc[(validation_set['label'] == 'ìš°ìš¸'), 'label'] = 5

train_set_data = [[i, str(j)] for i, j in zip(train_set['data'], train_set['label'])]

# validation_set_data = [[i, str(j)] for i, j in zip(validation_set['data'], validation_set['label'])]

train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=4)
train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)
test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)
train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=2)
test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=2)

```

### ğŸ§¸ ëª¨ë¸ í•™ìŠµ

```
model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)
# Prepare optimizer and schedule (linear warmup and decay)
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()
t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)

for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()  # Update learning rate schedule
        train_acc += calc_accuracy(out, label)
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
    print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
    model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        test_acc += calc_accuracy(out, label)
    print("epoch {} test acc {}".format(e+1, test_acc / (batch_id+1)))

```

**train ê³¼ validation ì§„í–‰**

### **í•™ìŠµ ì„¤ì •ê°’**

- BERTClassifier í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ëª¨ë¸ì€ BERT ëª¨ë¸ê³¼ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ë¡œ êµ¬ì„±
- AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
- ê°€ì¤‘ì¹˜ ê°ì‡ (weight decay)ë¥¼ ì ìš©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
- ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì ìš©í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ì™€ ì ìš©í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ë¶„í•˜ì—¬ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •
- CrossEntropyLoss í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

### **í•™ìŠµ ë™ì‘ê³¼ì • (for ~)**

- ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
- token_ids, segment_ids, valid_length, label ê°’ì„ GPU ì˜¬ë¦¬ê³ , modelì— input -> out
- out ê°’ê³¼ label ê°’ì„ ì‚¬ìš©í•˜ì—¬ Cross Entropy Lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
- loss.backward() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ëª¨ë¸ì˜ ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°
- optimizer.step() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ ê°’ì„ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
- CosineAnnealingWarmRestarts ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©, í•™ìŠµë¥ ì„ ì¡°ì •
- í˜„ì¬ ë°°ì¹˜ì˜ í•™ìŠµ ì •í™•ë„ train_accì— ëˆ„ì 
- í˜„ì¬ epochì˜ í•™ìŠµ ì •í™•ë„ë¥¼ ì¶œë ¥

### **ğŸ§¬ ê°ì •**
**ğŸ˜€ì¼ìƒ - 0 ğŸ˜Šê¸°ì¨ -1 ğŸ˜§ë¶ˆì•ˆ - 2 ğŸ˜­ìŠ¬í”” - 3 ğŸ˜¡ë¶„ë…¸ - 4 ğŸ˜¥ìš°ìš¸ - 5**

<img src = "https://user-images.githubusercontent.com/109534450/229018209-f9fb7af0-0800-47f8-981b-3ea3e13d2e9d.png" width="55%" height="55%">


### ğŸ“¤ Predict

```
def calc_accuracy(X,Y):
    max_vals, max_indices = torch.max(X, 1)
    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
    return train_acc
def predict(sentence):
    dataset = [[sentence, '0']]
    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)
    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=2)
    model.eval()
    answer = 0
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        for logits in out:
            logits = logits.detach().cpu().numpy()
            answer = np.argmax(logits)
    return answer
```

- predict í•¨ìˆ˜ëŠ” ì…ë ¥ ë¬¸ì¥ì„ ì…ë ¥ ë°›ì•„ í•´ë‹¹ ë¬¸ì¥ ê°ì • ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
- ì…ë ¥ ë¬¸ì¥ì„ datasetì— ì¶”ê°€í•˜ì—¬ BERTDataset ê°ì²´ë¥¼ ìƒì„±
- ëª¨ë¸ í‰ê°€ ëª¨ë“œ(model.eval())
- DataLoaderì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
- ëª¨ë¸ ì¶œë ¥ ê°’ì—ì„œ ê°€ì¥ í° ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ ê°’ìœ¼ë¡œ ë°˜í™˜

### âœ¨ ê²°ê³¼ë¬¼
**Loss**
- CrossEntropyLoss() - êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
- <img src = "https://user-images.githubusercontent.com/109534450/229016538-98df19eb-1bd3-4f78-8aff-abf498ca8759.png" width="55%" height="55%">
- ë‘Â í™•ë¥ Â ë¶„í¬ì˜Â ì°¨ì´ë¥¼Â êµ¬í•˜ê¸°Â ìœ„í•´ì„œÂ ì‚¬ìš©ë©ë‹ˆë‹¤.
- ì‹¤ì œÂ ë°ì´í„°ì˜Â í™•ë¥ Â ë¶„í¬ì™€, í•™ìŠµëœ ëª¨ë¸ì´Â ê³„ì‚°í•œ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.

<img src = "https://user-images.githubusercontent.com/109534450/229017302-99ac6324-ecd4-4e9c-8814-89a85b154cdd.png" width="55%" height="55%">



**Accuracy**


<img src = "https://user-images.githubusercontent.com/109534450/229020020-d61be612-8721-4348-9961-390a18955766.png" width="55%" height="55%">

### **ğŸ”§ ê°œë°œ í™˜ê²½**
- Google Colab
- Jupyter Hub






ì±—ë´‡
### **ë°ì´í„° ì…‹**
- AIÂ HubÂ ì œê³µ,Â ì›°ë‹ˆìŠ¤Â ëŒ€í™”Â ìŠ¤í¬ë¦½íŠ¸Â ë°ì´í„°ì…‹ (_í˜„ì¬ í˜ì´ì§€ ì—†ìŒ_)

---
## ğŸ’¡ **ê°ì„± ì±—ë´‡ ğŸ‘‰ KoBERT / KoGPT2**


