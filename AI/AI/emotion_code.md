# Emotion code

## ğŸ’¡ **ê°ì • ë¶„ì„**

::: warning

- BERTì˜ ì¥ì ì¸ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- BERTëŠ” ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ì— ëŒ€í•œ ì‚¬ì „ í•™ìŠµì´ ê°€ëŠ¥í•˜ë¯€ë¡œ, ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.
:::
 
### âœï¸**Process**

1. **ì…ë ¥ ë°ì´í„° ì „ ì²˜ë¦¬**
    - ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ í† í° IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë¬¸ì¥ì˜ ì‹¤ì œ ê¸¸ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ë¬¸ì¥ ìŒì˜ ê²½ìš°, ë¬¸ì¥ ê°„ êµ¬ë¶„ì„ ë‚˜íƒ€ë‚´ëŠ” ì„¸ê·¸ë¨¼íŠ¸ IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. **BERT ëª¨ë¸ ì ìš©**
    - ì „ì²˜ë¦¬ëœ ì…ë ¥ ë°ì´í„°ë¥¼ BERT ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì¶œë ¥ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
    - BERT ëª¨ë¸ì€ ì…ë ¥ í† í°ì„ ì„ë² ë”©í•˜ê³ , ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê° í† í°ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    - ì¶œë ¥ê°’ì€ BERTì˜ ì¶œë ¥ ì°¨ì› í¬ê¸°ì¸ 768ì°¨ì›ìœ¼ë¡œ ë‚˜ì˜µë‹ˆë‹¤.
3. **ì¶œë ¥ê°’ ë³€í™˜**
    - BERT ëª¨ë¸ì˜ ì¶œë ¥ê°’ ì¤‘ ë¬¸ì¥ì„ ëŒ€í‘œí•˜ëŠ” poolerë¥¼ ì‚¬ìš©í•˜ì—¬ 768ì°¨ì› ë²¡í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    - ë²¡í„°ë¥¼ ì„ í˜• ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ë ˆì´ë¸”ì— ëŒ€í•œ ë¡œì§“ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ë¡œì§“ì€ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
4. **ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™”**
    - logitê³¼ ì‹¤ì œ í´ë˜ìŠ¤ ë ˆì´ë¸” ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
    - í•™ìŠµì—ëŠ” í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent)ì˜ ë³€í˜• ì¤‘ í•˜ë‚˜ì¸ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - í•™ìŠµë¥ ê³¼ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ë“±ì„ ì¡°ì •í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.

### :alien: Dataset

- ê°ì • ëŒ€í™” ë§ë­‰ì¹˜ (AI í—ˆë¸Œ)
- ì†¡ì˜ìˆ™ ë°ì´í„°
- ì´ 6ë§Œ data

![pic](https://yogurt-bucket.s3.ap-northeast-2.amazonaws.com/Untitled+(2).png)
### ğŸ›  Requirement packege

```
!pip install gluonnlp pandas tqdm
!pip install mxnet
!pip install sentencepiece
!pip install transformers
!pip install torch
!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'

```

### âš¾ï¸ Import

```
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook
import pandas as pd
from sklearn.model_selection import train_test_split
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

from google.colab import drive
drive.mount('/content/drive')

```

- **torch**
- **nn, optim, F, Dataset, DataLoader :** íŒŒì´í† ì¹˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í•˜ìœ„ ëª¨ë“ˆ, ì‹ ê²½ë§ ëª¨ë¸, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í•¨ìˆ˜, ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë”
- **gluonnlp :** MXNet í”„ë ˆì„ ì›Œí¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **numpy :** ë‹¤ì°¨ì› í–‰ë ¬ ê´€ë ¨
- **pandas :** ë°ì´í„° ì²˜ë¦¬
- **KoBERTTokenizer :** KoBERTì˜ í† í¬ë‚˜ì´ì €
- **BertModel**
- **AdamW :** AdamW ìµœì í™”
- **get_cosine_schedule_with_warmup :** learning rate ìŠ¤ì¼€ì¤„ë§ ì ìš© í•¨ìˆ˜
- **drive.mount('/content/drive/') :** êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ Colabì— ë™ê¸°í™”
- **device = torch.device("cuda:0") :** GPU ì‚¬ìš©

### ğŸ¾ íŒŒë¼ë¯¸í„°

```
# Setting parameters
max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 20
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5
```

***íŒŒë¼ë¯¸í„° ì…‹íŒ…***

- **max_len** ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´
- **batch_size**
- **num_epochs :** ì „ì²´ í•™ìŠµ ë°ì´í„° í•™ìŠµ íšŸìˆ˜
- **learning_rate :** í•™ìŠµìœ¨

### ğŸ§šâ€â™€ï¸ BERT Dataset

```
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,
                 pad, pair):

        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))
```

- gluonnlpì˜ BERTSentenceTransformì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë¬¸ì¥ì„ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
-transform ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì¥ì„ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜

### ğŸ§šâ€â™‚ï¸ BERT Classifier

```
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=6,
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate

        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)

    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)

        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)

```

- ì…ë ¥ ë°ì´í„°ì˜ íŒ¨ë”© ë¶€ë¶„ì„ ì œì™¸í•˜ê³ , ì‹¤ì œ ì…ë ¥ì— ëŒ€í•œ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
- token_idsëŠ” ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•œ ê²°ê³¼
- gen_attention_mask ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±
- BERT ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ì—¬ ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

### ğŸ”« í† í¬ ë‚˜ì´ì € / ëª¨ë¸ ì •ì˜

```
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')
tok = tokenizer.tokenize
```

- skt/kobert-base-v1 ëª¨ë¸ì˜ **ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜** ë¡œë“œ
- BERTVocab ê°ì²´ì— tokenizer.vocab_file ì‚¬ì „(vocab) ë¡œë“œ

### ğŸ’¿ ë°ì´í„° ë¡œë“œ/ ì „ì²˜ë¦¬

```
train_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…¡á†·á„Œá…¥á†¼á„‡á…®á†«á„‰á…¥á†¨dataset.csv', encoding='cp949')
validation_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Validation.csv',encoding='cp949')
train_set = train_set.loc[:, ['sentiment', 'user']]
validation_set = validation_set.loc[:, ['ê°ì •_ëŒ€ë¶„ë¥˜', 'ì‚¬ëŒë¬¸ì¥1']]

train_set.dropna(inplace=True)
validation_set.dropna(inplace=True)
train_set.columns = ['label', 'data']
validation_set.columns = ['label', 'data']

train_set.loc[(train_set['label'] == 'ì¼ìƒ'), 'label'] = 0
train_set.loc[(train_set['label'] == 'ë¶„ë…¸'), 'label'] = 1
train_set.loc[(train_set['label'] == 'ë¶ˆì•ˆ'), 'label'] = 2
train_set.loc[(train_set['label'] == 'ìŠ¬í””'), 'label'] = 3
train_set.loc[(train_set['label'] == 'ê¸°ì¨'), 'label'] = 4
train_set.loc[(train_set['label'] == 'ìš°ìš¸'), 'label'] = 5

validation_set.loc[(validation_set['label'] == 'ì¼ìƒ'), 'label'] = 0
validation_set.loc[(validation_set['label'] == 'ë¶„ë…¸'), 'label'] = 1
validation_set.loc[(validation_set['label'] == 'ë¶ˆì•ˆ'), 'label'] = 2
validation_set.loc[(validation_set['label'] == 'ìŠ¬í””'), 'label'] = 3
validation_set.loc[(validation_set['label'] == 'ê¸°ì¨'), 'label'] = 4
validation_set.loc[(validation_set['label'] == 'ìš°ìš¸'), 'label'] = 5

train_set_data = [[i, str(j)] for i, j in zip(train_set['data'], train_set['label'])]

# validation_set_data = [[i, str(j)] for i, j in zip(validation_set['data'], validation_set['label'])]

train_set_data, test_set_data = train_test_split(train_set_data, test_size = 0.2, random_state=4)
train_set_data = BERTDataset(train_set_data, 0, 1, tok, vocab, max_len, True, False)
test_set_data = BERTDataset(test_set_data, 0, 1, tok, vocab, max_len, True, False)
train_dataloader = torch.utils.data.DataLoader(train_set_data, batch_size=batch_size, num_workers=2)
test_dataloader = torch.utils.data.DataLoader(test_set_data, batch_size=batch_size, num_workers=2)

```

### ğŸ§¸ ëª¨ë¸ í•™ìŠµ

```
model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)
# Prepare optimizer and schedule (linear warmup and decay)
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()
t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)

for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()  # Update learning rate schedule
        train_acc += calc_accuracy(out, label)
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
    print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
    model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        test_acc += calc_accuracy(out, label)
    print("epoch {} test acc {}".format(e+1, test_acc / (batch_id+1)))

```

**train ê³¼ validation ì§„í–‰**

### **ì„¤ì •**

- BERTClassifier í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ëª¨ë¸ì€ BERT ëª¨ë¸ê³¼ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ë¡œ êµ¬ì„±
- AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
- ê°€ì¤‘ì¹˜ ê°ì‡ (weight decay)ë¥¼ ì ìš©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
- ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì ìš©í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ì™€ ì ìš©í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ êµ¬ë¶„í•˜ì—¬ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •
- CrossEntropyLoss í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

### **í•™ìŠµ (for ~)**

- ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
- token_ids, segment_ids, valid_length, label ê°’ì„ GPU ì˜¬ë¦¬ê³ , modelì— input -> out
- out ê°’ê³¼ label ê°’ì„ ì‚¬ìš©í•˜ì—¬ Cross Entropy Lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
- loss.backward() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ëª¨ë¸ì˜ ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°
- optimizer.step() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ ê°’ì„ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
- CosineAnnealingWarmRestarts ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©, í•™ìŠµë¥ ì„ ì¡°ì •
- í˜„ì¬ ë°°ì¹˜ì˜ í•™ìŠµ ì •í™•ë„ train_accì— ëˆ„ì 
- í˜„ì¬ epochì˜ í•™ìŠµ ì •í™•ë„ë¥¼ ì¶œë ¥

### **í‰ê°€**

- ëª¨ë¸ í‰ê°€ ëª¨ë“œ
- ìœ„ì™€ ë§ˆì°¬ê°€ì§€. ì •í™•ë„ ì¶œë ¥

### ğŸ“¤ Predict

```
def calc_accuracy(X,Y):
    max_vals, max_indices = torch.max(X, 1)
    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
    return train_acc
def predict(sentence):
    dataset = [[sentence, '0']]
    test = BERTDataset(dataset, 0, 1, tok, vocab, max_len, True, False)
    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=2)
    model.eval()
    answer = 0
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        for logits in out:
            logits = logits.detach().cpu().numpy()
            answer = np.argmax(logits)
    return answer
```

- predict í•¨ìˆ˜ëŠ” ì…ë ¥ ë¬¸ì¥ì„ ì…ë ¥ ë°›ì•„ í•´ë‹¹ ë¬¸ì¥ ê°ì • ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
- ì…ë ¥ ë¬¸ì¥ì„ datasetì— ì¶”ê°€í•˜ì—¬ BERTDataset ê°ì²´ë¥¼ ìƒì„±
- ëª¨ë¸ í‰ê°€ ëª¨ë“œ(model.eval())
- DataLoaderì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ ê°’ì„ ì–»ìŠµë‹ˆë‹¤.
- ëª¨ë¸ ì¶œë ¥ ê°’ì—ì„œ ê°€ì¥ í° ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ ê°’ìœ¼ë¡œ ë°˜í™˜

### ğŸ“© OUT

```
sentence = "ì˜¤ëŠ˜ ê°‘ìê¸° ë‚ ì”¨ê°€ ì¶”ì›Œì ¸ì„œ ë†€ëì–´"
predict(sentence) # 0 -> ì¼ìƒ
```
### ëª¨ë¸ í‰ê°€
- **Confusion matrix**
```python
# Confusion Matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns
label_map = {'ì¼ìƒ': 0, 'ê¸°ì¨': 1, 'ë¶ˆì•ˆ': 2, 'ìŠ¬í””': 3, 'ë¶„ë…¸': 4, 'ìš°ìš¸': 5}
def label_to_number(labels, label_map):
    return [label_map[label] for label in labels]
model.eval()
y_true = [] 
y_pred = []
with torch.no_grad():
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        _, predicted = torch.max(out.data, 1)
        y_true.extend(label.tolist())
        y_pred.extend(predicted.tolist())
    # y_true = label_to_number(y_true, label_map)
    # y_pred = label_to_number(y_pred, label_map)
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
```
- **loss**
```python
import matplotlib.pyplot as plt

plt.plot(X,Y)
plt.title('Model Loss')
plt.xlabel('steps')
plt.ylabel('loss')
plt.show()

plt.plot(X, train_acces, label='Train Accuracy')
plt.plot(X, test_acces, label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Train and Test Accuracy over Epochs')
plt.legend()
plt.show()
```